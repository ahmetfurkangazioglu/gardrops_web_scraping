# Gardrops Web Scraping Projesi

Gardrops.com'dan Ã§anta kategorisindeki Ã¼rÃ¼n bilgilerini Ã§eken Python web scraping projesi.

## ğŸ¯ Ã–zellikler

- 50 sayfa Ã¼zerinden 3,500+ Ã¼rÃ¼n verisi Ã§ekme
- Selenium ile dinamik iÃ§erik yÃ¶netimi
- 13 farklÄ± Ã¼rÃ¼n Ã¶zelliÄŸini toplama:
  - ÃœrÃ¼n AdÄ±, Fiyat, Marka
  - Kategori, KullanÄ±m Durumu, Renk
  - Ä°lan Tarihi, AÃ§Ä±klama
  - SatÄ±cÄ± Bilgisi, GÃ¶rsel URL
  - BeÄŸeni ve GÃ¶rÃ¼ntÃ¼lenme SayÄ±sÄ±

## ğŸ“¦ Gereksinimler

```bash
pip install selenium beautifulsoup4 pandas openpyxl
```

## ğŸš€ KullanÄ±m

1. Jupyter Notebook'u aÃ§Ä±n
2. TÃ¼m hÃ¼creleri sÄ±rayla Ã§alÄ±ÅŸtÄ±rÄ±n
3. `scrape_gardrops_with_selenium()` fonksiyonunu istediÄŸiniz parametrelerle kullanÄ±n
4. SonuÃ§lar otomatik olarak Excel dosyasÄ±na kaydedilir

## ğŸ“Š Ã–rnek Ã‡Ä±ktÄ±

- **Format:** Excel (.xlsx)
- **Dosya adÄ±:** `gardrops_cantalar_[SAYFA_SAYISI]_SAYFA_[TARIH].xlsx`
- **Ã–rnek:** 3,574 Ã¼rÃ¼n, 144 marka, 11 kategori

## âš™ï¸ Ayarlar

- `max_pages`: KaÃ§ sayfa scraping yapÄ±lacak (varsayÄ±lan: 50)
- `max_products_per_page`: Sayfa baÅŸÄ±na maksimum Ã¼rÃ¼n (varsayÄ±lan: tÃ¼mÃ¼)

## ğŸ“ Notlar

- Selenium ChromeDriver kullanÄ±r (headless mod)
- Rate limiting iÃ§in Ã¼rÃ¼nler arasÄ± 1 saniye bekleme
- Sayfa yÃ¼klemeleri iÃ§in 3 saniye render sÃ¼resi

## ğŸ› ï¸ Teknik Detaylar

- **Next.js** tabanlÄ± site iÃ§in Ã¶zel regex parsing
- Escaped JSON formatÄ±ndan veri Ã§Ä±karma
- Lazy loading iÃ§in otomatik scroll
- Hata yÃ¶netimi ve loglama

## ğŸ“„ Lisans

Bu proje kiÅŸisel kullanÄ±m iÃ§indir.
